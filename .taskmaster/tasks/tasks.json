{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Design Core Triple Store & HLC Mechanism",
        "description": "Define the fundamental data model for the triple store (subject, predicate, object) and the structure/logic for Hybrid Logical Clocks (HLC) at the field level.",
        "details": "This task involves designing the in-memory representation of the triple store, including indexing strategies (e.g., by subject, predicate, object), and the HLC timestamp format (e.g., (logical_time, wall_clock_time, node_id)). It also covers the basic principles of HLC generation and comparison for conflict resolution.",
        "testStrategy": "Document design specifications. Conduct design reviews with relevant stakeholders. Create mock data structures to validate the model's flexibility and efficiency for HLC integration.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Core Triple Data Model Structure",
            "description": "Specify the fundamental structure of a triple (subject, predicate, object) and the allowed internal data types for each component.",
            "dependencies": [],
            "details": "This involves defining the internal representation of subjects, predicates, and objects, considering potential UUIDs, strings, or other identifiers for efficient storage and retrieval. It also covers the conceptual schema for a triple.\n<info added on 2025-08-29T02:16:10.669Z>\nInitial core data model types (`Triple`, `Subject`, `Predicate`, `Object`, `Value`, `Ref`, `RefMany`, and a placeholder for `HLC` as a string) have been defined in `index.ts`.\n</info added on 2025-08-29T02:16:10.669Z>",
            "status": "done",
            "testStrategy": "Document the data model specification. Conduct design reviews with data architects and system designers."
          },
          {
            "id": 2,
            "title": "Design In-Memory Triple Store Representation",
            "description": "Design the optimal in-memory data structures for storing triples efficiently, prioritizing fast read and write operations.",
            "dependencies": [
              "1.1"
            ],
            "details": "This involves selecting appropriate data structures (e.g., nested hash maps, arrays, custom structures) to hold the triples, considering memory footprint and access patterns.\n<info added on 2025-08-29T02:26:34.861Z>\nThe in-memory TripleStore has been implemented using a class-based approach, incorporating SPO, POS, and OSP indexes to facilitate efficient querying. It supports adding, removing, and querying triples, including those with partial patterns.\n</info added on 2025-08-29T02:26:34.861Z>",
            "status": "done",
            "testStrategy": "Document the in-memory structure design. Create mock data structures to validate memory efficiency and basic access patterns. Conduct design reviews."
          },
          {
            "id": 3,
            "title": "Design Subject-Predicate-Object Indexing Strategy",
            "description": "Define the indexing strategies for efficient retrieval of triples based on subject, predicate, and object components.",
            "dependencies": [
              "1.2"
            ],
            "details": "This includes designing specific index structures (e.g., SPO, POS, OSP permutations) to support various query patterns and ensure rapid lookup times.\n<info added on 2025-08-29T02:26:45.480Z>\nImplemented SPO, POS, and OSP indexing within the TripleStore class to allow for efficient querying of triples by any combination of subject, predicate, and object.\n</info added on 2025-08-29T02:26:45.480Z>",
            "status": "done",
            "testStrategy": "Document the indexing strategy. Simulate common query patterns against proposed index structures to estimate performance. Conduct design reviews."
          },
          {
            "id": 4,
            "title": "Define Hybrid Logical Clock (HLC) Timestamp Format",
            "description": "Specify the exact structure and components of the HLC timestamp, including logical time, wall clock time, and node identifier.",
            "dependencies": [],
            "details": "This involves defining the data types, precision, and ranges for each component of the HLC timestamp (e.g., 64-bit integers for logical and wall clock time, a unique identifier for node ID).\n<info added on 2025-08-29T02:19:21.933Z>\nImplemented HLC creation, parsing, incrementing, and comparison functions in `index.ts`. Added a full test suite in `index.test.ts` using `bun:test` and all tests are passing.\n</info added on 2025-08-29T02:19:21.933Z>\n<info added on 2025-08-29T02:25:10.622Z>\n<info added on 2024-07-30T12:00:00.000Z>\nRefactored the HLC implementation from a string-based approach to a class-based, object-oriented design. The HLC class now encapsulates all related logic, improving maintainability and type safety.\n</info added on 2024-07-30T12:00:00.000Z>\n</info added on 2025-08-29T02:25:10.622Z>",
            "status": "pending",
            "testStrategy": "Document the HLC timestamp format specification. Conduct design reviews to ensure compatibility and uniqueness requirements are met."
          },
          {
            "id": 5,
            "title": "Design HLC Generation Logic",
            "description": "Define the algorithm for generating new HLC timestamps for local operations and when merging with remote timestamps.",
            "dependencies": [
              "1.4"
            ],
            "details": "This includes specifying rules for incrementing the logical time component, updating the wall clock time component, and handling potential clock skew during generation and merge operations.",
            "status": "done",
            "testStrategy": "Document the HLC generation algorithm. Create pseudocode or flowcharts to illustrate the logic. Conduct design reviews."
          },
          {
            "id": 6,
            "title": "Design HLC Comparison Logic",
            "description": "Define the rules and algorithm for comparing two HLC timestamps to determine causality or precedence.",
            "dependencies": [
              "1.4"
            ],
            "details": "This involves specifying how the logical time, wall clock time, and node ID components are used in the comparison process to establish a total or partial order.",
            "status": "done",
            "testStrategy": "Document the HLC comparison algorithm. Define edge cases for comparison and verify logic. Conduct design reviews."
          },
          {
            "id": 7,
            "title": "Design Field-Level HLC Application within Triples",
            "description": "Specify precisely how HLC timestamps are associated with individual fields (objects) within the triple store for fine-grained versioning and conflict resolution.",
            "dependencies": [
              "1.1",
              "1.4"
            ],
            "details": "This involves deciding whether the HLC is embedded within the object value, stored as a separate metadata field alongside each triple, or managed through a versioning layer.\n<info added on 2025-08-29T02:44:45.628Z>\nIt has been decided that the HLC will be stored as a fourth element within each triple, forming `[Subject, Predicate, Object, HLC]`. This structure inherently supports field-level HLC application, where each triple's HLC timestamps the specific field (predicate) for a given subject. This design satisfies the requirement for fine-grained versioning and conflict resolution at the field level, and no further dedicated implementation is required for this aspect.\n</info added on 2025-08-29T02:44:45.628Z>",
            "status": "done",
            "testStrategy": "Document the HLC application strategy. Create mock data structures to illustrate HLC placement and association. Conduct design reviews."
          },
          {
            "id": 8,
            "title": "Design HLC-based Conflict Resolution Strategy",
            "description": "Define the primary conflict resolution strategy using HLCs, specifically focusing on 'last-write-wins' at the field level.",
            "dependencies": [
              "1.6",
              "1.7"
            ],
            "details": "This involves outlining the precise logic for selecting the 'winning' triple based on HLC comparison when concurrent updates to the same field occur, ensuring deterministic outcomes.\n<info added on 2025-08-29T02:48:19.440Z>\nWhen adding a triple, if a triple with the same subject, predicate, and object already exists, the HLCs are compared, and only the triple with the highest HLC is retained. This ensures data consistency in the face of concurrent updates.\n</info added on 2025-08-29T02:48:19.440Z>",
            "status": "done",
            "testStrategy": "Document the conflict resolution algorithm. Define various conflict scenarios and demonstrate how the HLC-based strategy resolves them. Conduct design reviews."
          },
          {
            "id": 9,
            "title": "Design for Triple Store Mutability and Versioning",
            "description": "Determine how updates to triples are handled, considering whether triples are immutable (new version created) or mutable (in-place update), and how HLCs integrate with this.",
            "dependencies": [
              "1.2",
              "1.7"
            ],
            "details": "This impacts how historical versions might be retained or discarded based on HLCs and the overall data lifecycle within the in-memory store.\n<info added on 2025-08-29T02:54:39.255Z>\nThe store is designed as an immutable, versioned TripleStore. For any given subject and predicate, only the triple with the highest HLC timestamp is retained, implementing a 'last-write-wins' strategy to ensure the store always reflects the latest version of a fact.\n</info added on 2025-08-29T02:54:39.255Z>",
            "status": "done",
            "testStrategy": "Document the mutability/versioning model. Illustrate update flows with HLCs. Conduct design reviews."
          },
          {
            "id": 10,
            "title": "Design for Data Type Representation within Triples",
            "description": "Define how various client-side data types (string, number, boolean, date, ref, refMany) are represented and stored as objects within the triple store.",
            "dependencies": [
              "1.1"
            ],
            "details": "This includes considerations for serialization/deserialization, efficient storage, and type integrity for different data types when they form the object component of a triple.\n<info added on 2025-08-29T02:38:57.090Z>\nA prefix-based encoding system has been designed and implemented in `crdt.ts` to represent various data types (strings, numbers, booleans, dates, and null values) for the object component of triples. This system is fully tested.\n</info added on 2025-08-29T02:38:57.090Z>\n<info added on 2025-08-29T02:44:01.399Z>\n<info added on 2025-08-29T02:38:57.090Z>\nPer user feedback, the decision has been made to store primitive data types directly in memory without an encoding layer. The previously designed and implemented prefix-based encoding system in `crdt.ts` is being removed.\n</info added on 2025-08-29T02:38:57.090Z>\n</info added on 2025-08-29T02:44:01.399Z>",
            "status": "done",
            "testStrategy": "Document the data type representation mapping. Create examples of how different types are stored. Conduct design reviews."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Client-side Schema Definition (`createSchema`)",
        "description": "Develop the `createSchema` utility for client-only schema definition, enabling type safety and schema evolution through field-level storage and fallbacks.",
        "details": "The utility should support defining entities and rooms with specific field types (t.string, t.number, t.boolean, t.date, t.ref, t.refMany). It needs to parse the schema, generate internal type definitions, and provide mechanisms for schema validation during local operations.",
        "testStrategy": "Unit tests for `createSchema` parsing and validation logic. Integration tests to ensure defined schemas correctly influence data type handling and error reporting for invalid data.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement `createSchema` Core & Type Parsing",
            "description": "Develop the foundational `createSchema` utility, enabling the definition of entities and rooms. This subtask includes parsing the input schema definition (e.g., `t.string`, `t.number`, `t.ref`) and converting it into a robust internal data structure suitable for type checking and further processing.",
            "dependencies": [],
            "details": "Define the internal representation for schema types. Implement the parsing logic within `createSchema` to transform user-defined schema objects into this internal format. Ensure support for `t.string`, `t.number`, `t.boolean`, `t.date`, `t.ref`, `t.refMany`.\n<info added on 2025-08-29T03:00:04.746Z>\nThe internal, optimized Map representation of schema definitions, established by the `createSchema` utility, is now available within the `Schema` object to serve as a strong foundation for efficient validation and type-checking.\n</info added on 2025-08-29T03:00:04.746Z>\n<info added on 2025-08-29T03:04:34.497Z>\nThe schema definition now includes support for `fallback` and `optional` field options, and the internal representation generated by `createSchema` fully encompasses both `entities` and `rooms` structures, which the validation logic must now robustly handle.\n</info added on 2025-08-29T03:04:34.497Z>",
            "status": "done",
            "testStrategy": "Unit tests for `createSchema` input parsing, ensuring correct internal representation for various valid and invalid schema definitions."
          },
          {
            "id": 2,
            "title": "Develop Schema Validation Logic",
            "description": "Implement the core validation engine that uses the internally defined schema to enforce type safety during local data operations (e.g., create, update). This mechanism will ensure that data conforms to the defined types and structures.",
            "dependencies": [
              "2.1"
            ],
            "details": "Design and implement validation functions that can be invoked when data is written or updated. These functions should check field types, required fields, and reference integrity based on the parsed schema.\n<info added on 2025-08-29T03:15:38.278Z>\nImplemented the `Schema.validate` method for robust runtime validation of objects against the schema. This includes checks for correct field types, required fields, and the proper structure of `ref` and `refMany` reference types, ensuring data integrity. The functionality is fully tested.\n</info added on 2025-08-29T03:15:38.278Z>",
            "status": "done",
            "testStrategy": "Unit tests for validation rules against various data inputs (valid, invalid types, missing required fields, incorrect references). Integration tests to confirm validation errors are correctly reported during mock data operations."
          },
          {
            "id": 3,
            "title": "Design & Implement Schema Versioning",
            "description": "Develop a mechanism to support schema evolution, including versioning of schemas and handling field-level storage. This allows for backward and forward compatibility as the application's data model changes over time.",
            "dependencies": [
              "2.1"
            ],
            "details": "Define how schema versions will be managed and associated with data. Implement logic to store schema version metadata alongside data or within the schema definition itself. Consider strategies for migrating data between schema versions if necessary.",
            "status": "done",
            "testStrategy": "Unit tests for schema version assignment and retrieval. Integration tests to verify that data can be associated with specific schema versions."
          },
          {
            "id": 4,
            "title": "Implement Field-Level Fallbacks",
            "description": "Add support for defining and applying field-level fallbacks within the schema. This feature provides default values or alternative logic when a field is missing or invalid, enhancing data robustness and schema evolution.",
            "dependencies": [
              "2.1"
            ],
            "details": "Extend the schema definition to allow specifying fallback values or functions for individual fields. Implement the runtime logic to apply these fallbacks during data retrieval or validation if a field's value is not present or does not conform.",
            "status": "done",
            "testStrategy": "Unit tests for various fallback scenarios (missing fields, invalid types triggering fallback). Integration tests to ensure fallbacks are correctly applied when querying data."
          },
          {
            "id": 5,
            "title": "Document `createSchema` Utility & Examples",
            "description": "Create comprehensive developer documentation for the `createSchema` utility, covering its API, usage patterns, best practices, and detailed examples for defining entities, rooms, and various field types.",
            "dependencies": [
              "2.1",
              "2.2",
              "2.3",
              "2.4"
            ],
            "details": "Write API reference documentation for `createSchema` and its associated type helpers (`t.string`, `t.ref`, etc.). Include examples demonstrating schema definition, validation implications, and how to leverage versioning and fallbacks.",
            "status": "done",
            "testStrategy": "Review documentation for clarity, completeness, and accuracy, ensuring all features are adequately explained with practical examples."
          }
        ]
      },
      {
        "id": 3,
        "title": "Develop Low-level Client-side In-memory CRUD Operations",
        "description": "Implement the core create, read, update, and delete operations directly on the client's in-memory triple store, incorporating HLC for local conflict resolution.",
        "details": "This involves the internal logic for manipulating triples, generating HLC timestamps for new or updated fields, and applying last-write-wins conflict resolution for same-field concurrent updates locally. Operations should be instant and provide immediate feedback.",
        "testStrategy": "Unit tests for each CRUD operation (create, update, delete, read). Concurrency tests to simulate local concurrent writes to the same field and verify HLC-based last-write-wins behavior. Performance tests for basic operations on a small dataset.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "in-progress",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Client Factory & In-memory Triple Store",
            "description": "Set up the client factory, initialize the in-memory triple store, and integrate the HLC mechanism for timestamp generation and local conflict resolution. This forms the foundational layer for all subsequent CRUD operations.",
            "dependencies": [],
            "details": "Implement the `createClient` factory function. Establish the internal data structure for the in-memory triple store (e.g., `Map<subject, Map<predicate, {value, hlc_timestamp}>>`). Integrate the HLC utility (from Task 1) for generating timestamps for new/updated triples. Define the last-write-wins conflict resolution logic for same-field concurrent updates.\n<info added on 2025-08-29T03:25:31.257Z>\nThe `create` operation will be exposed via the dynamic `database` API, which is generated using a Proxy based on the provided schema. This API will be the entry point for creating new entities, ensuring schema validation and applying fallback logic as defined.\n</info added on 2025-08-29T03:25:31.257Z>",
            "status": "done",
            "testStrategy": "Unit tests for client factory instantiation, in-memory store initialization, and basic HLC timestamp generation and application during initial data insertion."
          },
          {
            "id": 2,
            "title": "Implement `create` Operation with Schema Validation & Fallbacks",
            "description": "Develop the `create` method for adding new entities to the in-memory store, ensuring schema validation and applying fallback values as defined in the schema.",
            "dependencies": [
              "3.1"
            ],
            "details": "Implement the `client.create()` method. Integrate with the schema definition utility (from Task 2) to validate incoming data against the defined schema. Apply fallback values for missing fields as per schema. Generate HLC timestamps for all new triples using the integrated HLC utility.\n<info added on 2025-08-29T03:32:27.880Z>\nThe `create` operation will be exposed via `client.database.<entityName>.create(fields)`. Validation must specifically enforce that all required fields are present. For each field in the final entity, a triple of the form `(<new_entity_id>`, `<entityName>/<fieldName>`, `<value>)` must be generated and stored. The operation should return the complete created entity, including auto-generated fields like `id`, `createdAt`, and `updatedAt`.\n</info added on 2025-08-29T03:32:27.880Z>",
            "status": "done",
            "testStrategy": "Unit tests for successful creation, schema validation errors, fallback application, and correct HLC timestamping."
          },
          {
            "id": 3,
            "title": "Implement `read` Operation for Entity Retrieval",
            "description": "Develop the `read` method to efficiently retrieve entities or specific fields from the client's in-memory triple store.",
            "dependencies": [
              "3.1"
            ],
            "details": "Implement `client.read()` to fetch entities by ID or query for specific fields. Ensure efficient retrieval from the in-memory store. Consider supporting basic filtering or projection if implied by `client-api.md`.\n<info added on 2025-08-29T03:32:34.558Z>\nThe `read` functionality will be exposed through `query` and `queryOne` methods on the entity-specific API (e.g., `client.database.users.query(...)`). This will involve implementing a Datalog-style query engine that operates on the in-memory triple store as described in `design.md`. The query engine must support filtering (`where`), field selection (`fields`), sorting (`orderBy`), and limiting results (`limit`). The implementation should gather all triples for matching subjects and reconstruct entity objects based on the `fields` selection.\n</info added on 2025-08-29T03:32:34.558Z>",
            "status": "done",
            "testStrategy": "Unit tests for retrieving existing entities, non-existent entities, specific fields, and handling various query parameters."
          },
          {
            "id": 4,
            "title": "Implement `update` & `replace` Operations with Functional Updates",
            "description": "Develop the `update` and `replace` methods to modify existing entities, supporting functional updates and applying HLC-based last-write-wins conflict resolution.",
            "dependencies": [
              "3.1",
              "3.3"
            ],
            "details": "Implement `client.update()` and `client.replace()`. Support passing functions to update fields (e.g., `count: c => c + 1`). Generate new HLC timestamps for updated fields using the integrated HLC utility. Apply last-write-wins logic based on HLC timestamps for concurrent updates to the same field.\n<info added on 2025-08-29T03:32:39.323Z>\nThe methods should be accessed via `client.database.<entityName>.update({ id, fields })` and `client.database.<entityName>.replace({ id, fields })`. For `update`, only provided fields are modified. For `replace`, all required fields must be provided, and unspecified optional fields are cleared. Both methods must support a functional `fields` argument of the form `(prev) => ({...})` for atomic updates. The last-write-wins conflict resolution strategy must be applied at the field level as specified in `design.md`.\n</info added on 2025-08-29T03:32:39.323Z>",
            "status": "done",
            "testStrategy": "Unit tests for direct updates, functional updates, `replace` behavior, and conflict resolution scenarios (last-write-wins verification)."
          },
          {
            "id": 5,
            "title": "Implement `delete` Operation for Entities/Fields",
            "description": "Develop the `delete` method to remove entities or specific fields from the in-memory triple store.",
            "dependencies": [
              "3.1"
            ],
            "details": "Implement `client.delete()` to remove entire entities by ID or specific predicates for an entity. Ensure immediate removal from the in-memory store.\n<info added on 2025-08-29T03:32:44.328Z>\nFor `client.database.<entityName>.delete(id)`, instead of immediate removal, a 'tombstone' triple `(<entityId>, \"_deleted\", true)` with an HLC timestamp must be created. The query engine must be updated to filter out entities with this tombstone, ensuring deletions are replicated and win over concurrent updates.\n</info added on 2025-08-29T03:32:44.328Z>",
            "status": "done",
            "testStrategy": "Unit tests for deleting existing entities, non-existent entities, specific fields, and verifying removal."
          },
          {
            "id": 6,
            "title": "Implement Bulk `updateMany` & `replaceMany` Operations",
            "description": "Develop bulk `updateMany` and `replaceMany` methods for efficient modification of multiple entities, leveraging the single update/replace logic.",
            "dependencies": [
              "3.4",
              "21"
            ],
            "details": "Implement `client.updateMany()` and `client.replaceMany()`. These methods should iterate over the provided entities/updates and call the respective single `update` or `replace` logic internally, ensuring HLC and conflict resolution are applied per entity/field. Optimize for batch processing where possible.\n<info added on 2025-08-29T03:32:48.983Z>\nImplement `client.database.<entityName>.updateMany({ where, fields })` and `client.database.<entityName>.replaceMany({ where, fields })`. These methods should first query the triple store to find all entities matching the `where` clause. For each matching entity, they should then apply the logic from the single `update` or `replace` operation internally, ensuring HLC and conflict resolution are applied per entity/field. Both `updateMany` and `replaceMany` should support functional `fields` arguments. The operations should be performed as a batch, and the return value should indicate the count of affected documents, as shown in `client-api.md`.\n</info added on 2025-08-29T03:32:48.983Z>",
            "status": "deferred",
            "testStrategy": "Unit tests for bulk updates/replaces on multiple entities, including mixed success/failure scenarios and performance considerations for large batches."
          },
          {
            "id": 7,
            "title": "Implement Bulk `deleteMany` Operation",
            "description": "Develop the bulk `deleteMany` method for efficient removal of multiple entities or fields from the in-memory store.",
            "dependencies": [
              "3.5",
              "21"
            ],
            "details": "Implement `client.deleteMany()`. This method should iterate over the provided entity IDs/field paths and call the single `delete` logic internally. Optimize for batch processing.\n<info added on 2025-08-29T03:32:58.082Z>\nThe method signature will be `client.database.<entityName>.deleteMany({ where })`. It will identify entities by querying the triple store using the `where` clause. Deletion will involve creating a 'tombstone' triple for each identified entity, consistent with the single `delete` operation. The method should return the total count of documents marked for deletion.\n</info added on 2025-08-29T03:32:58.082Z>",
            "status": "deferred",
            "testStrategy": "Unit tests for bulk deletes on multiple entities/fields, including partial success and performance."
          },
          {
            "id": 8,
            "title": "Comprehensive CRUD Testing & Performance Benchmarking",
            "description": "Conduct comprehensive unit, integration, and performance tests for all implemented in-memory CRUD operations, including HLC-based conflict resolution.",
            "dependencies": [
              "3.2",
              "3.3",
              "3.4",
              "3.5",
              "3.6",
              "3.7"
            ],
            "details": "Write extensive unit tests covering edge cases for each CRUD method. Develop integration tests to verify end-to-end data flow and consistency across multiple operations. Implement concurrency tests to simulate local concurrent writes to the same field and verify HLC-based last-write-wins behavior. Conduct performance benchmarks for basic operations on varying dataset sizes to ensure 'instant' feedback.\n<info added on 2025-08-29T03:33:06.169Z>\nThe test suite will be expanded to explicitly cover all CRUD operations (`create`, `query`, `update`, `replace`, `delete`, and their `many` variants). Tests will verify: correct triple manipulation in the in-memory store; schema validation and fallback application; functional updates executing correctly; HLC timestamp generation and last-write-wins conflict resolution for concurrent local updates; correct handling of delete tombstones in queries; and bulk operations affecting the correct set of documents. Performance benchmarks will specifically cover each CRUD operation on a reasonably sized in-memory dataset to ensure 'instant' feedback.\n</info added on 2025-08-29T03:33:06.169Z>",
            "status": "pending",
            "testStrategy": "As described in the details. Focus on robustness, correctness, and performance."
          },
          {
            "id": 9,
            "title": "Create and Use Custom `DatabaseError` Type",
            "description": "Define a custom `DatabaseError` class and update all error-throwing paths in the client to use this type instead of the generic `Error` class, as specified in the documentation.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 3
          },
          {
            "id": 10,
            "title": "Standardize CRUD Return Types to `DatabaseResult`",
            "description": "Refactor `update`, `replace`, and `delete` methods to return the `DatabaseResult<void>` type as specified in the client API documentation, instead of the current `Promise<{ error?: Error }>`. The `DatabaseResult` should not be a promise itself.",
            "details": "",
            "status": "done",
            "dependencies": [
              "3.9"
            ],
            "parentTaskId": 3
          },
          {
            "id": 11,
            "title": "Implement Functional Updates for `update` and `replace`",
            "description": "Add support for functional updates to the `update` and `replace` methods, allowing a function `(prev) => ({...})` to be passed as the `fields` argument for atomic operations.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 3
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement Client-side In-memory Query Engine (Basic)",
        "description": "Develop the client-side query engine to support fundamental query capabilities on the in-memory triple store, including `fields`, `where`, `orderBy`, and `limit` operators.",
        "details": "The query engine should efficiently retrieve data based on specified criteria. It needs to handle filtering by field values, selecting specific fields, ordering results, and limiting the number of returned documents. This is the internal query mechanism.",
        "testStrategy": "Unit tests for each query operator (`where`, `orderBy`, `limit`, `fields`). Integration tests to verify complex queries combining multiple operators return correct results from the in-memory store. Performance tests for queries on medium-sized datasets.",
        "priority": "high",
        "dependencies": [
          1,
          3,
          "23"
        ],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Query Language & AST for Basic Operators",
            "description": "Formalize the structure of basic client-side queries including `where`, `fields`, `orderBy`, and `limit` clauses. Define the Abstract Syntax Tree (AST) representation for these query types to enable structured parsing and execution.",
            "dependencies": [],
            "details": "This involves designing the JSON-like query object structure that clients will use, and mapping it to an internal AST that the parser and executor will understand. Consider how `where` clauses will handle basic comparisons like equality and range.",
            "status": "pending",
            "testStrategy": "N/A (design phase)"
          },
          {
            "id": 2,
            "title": "Implement Query Parser for Basic Operators",
            "description": "Develop a parser that takes a client-side query object (JSON) and transforms it into the defined Abstract Syntax Tree (AST). The parser should validate the query structure and parameters for `where`, `fields`, `orderBy`, and `limit`.",
            "dependencies": [
              "4.1"
            ],
            "details": "The parser will be responsible for converting the declarative query into an executable internal representation. It should include robust error handling for malformed queries or invalid parameters.",
            "status": "pending",
            "testStrategy": "Unit tests for parsing valid and invalid query structures for each operator and their combinations."
          },
          {
            "id": 3,
            "title": "Implement Core Query Execution Engine",
            "description": "Build the foundational execution engine responsible for traversing the query AST and applying the query logic to the in-memory triple store. This subtask focuses on the execution flow and data iteration mechanism, not the specific operator logic.",
            "dependencies": [
              "4.1",
              "4.2"
            ],
            "details": "The executor will receive the AST from the parser and orchestrate the application of `where`, `fields`, `orderBy`, and `limit` operations on the in-memory data. It should provide a flexible framework for integrating operator-specific logic.",
            "status": "pending",
            "testStrategy": "Unit tests for the executor's ability to process a simple AST, iterate over a dataset, and correctly pass data to placeholder operator functions."
          },
          {
            "id": 4,
            "title": "Implement `where`, `fields`, `orderBy`, and `limit` Operators",
            "description": "Develop the specific logic for each fundamental query operator: `where` (for basic filtering like equality, comparison, inclusion), `fields` (for projection), `orderBy` (for sorting), and `limit` (for result truncation).",
            "dependencies": [
              "4.3"
            ],
            "details": "For `where`, implement basic comparison operators such as `equals`, `notEquals`, `greaterThan`, `lessThan`, `in`, and `notIn`. Ensure efficient application of these operators within the executor framework, considering data types and indexing if applicable.",
            "status": "pending",
            "testStrategy": "Unit tests for each operator individually (e.g., `where` with various conditions, `fields` with different projections, `orderBy` with ascending/descending sorts, `limit` with various values including zero and exceeding dataset size)."
          },
          {
            "id": 5,
            "title": "Integrate Query Engine with Client API Methods",
            "description": "Connect the implemented query engine to the client-side API methods: `query`, `updateMany`, `replaceMany`, and `deleteMany`. Ensure these methods correctly utilize the query engine for data retrieval, filtering, and identification of target documents.",
            "dependencies": [
              "4.4"
            ],
            "details": "The `query` method will directly use the full query engine. `updateMany`, `replaceMany`, and `deleteMany` will primarily use the `where` clause functionality of the engine to select the documents to modify or delete.",
            "status": "pending",
            "testStrategy": "Integration tests to verify that `query`, `updateMany`, `replaceMany`, and `deleteMany` correctly use the query engine to perform their operations on the in-memory store."
          },
          {
            "id": 6,
            "title": "Add Comprehensive Query Engine Tests",
            "description": "Write extensive unit, integration, and performance tests for the client-side in-memory query engine. Cover all operators, their combinations, edge cases, and verify correct behavior with the in-memory triple store.",
            "dependencies": [
              "4.5"
            ],
            "details": "Include tests for complex queries combining multiple operators (`where` + `orderBy` + `limit`), edge cases (empty results, no matching criteria, large limits), and performance tests on medium-sized datasets as per the parent task's test strategy.",
            "status": "pending",
            "testStrategy": "Unit tests for each query operator (`where`, `orderBy`, `limit`, `fields`). Integration tests to verify complex queries combining multiple operators return correct results from the in-memory store. Performance tests for queries on medium-sized datasets."
          }
        ]
      },
      {
        "id": 5,
        "title": "Establish Client-side WebSocket & HTTP Connection Management",
        "description": "Set up robust client-side connection management for WebSocket (real-time updates) and HTTP (bulk operations, initial data loading) for replication.",
        "details": "This includes establishing and maintaining connections, handling disconnections, basic reconnection logic, and managing data flow over these protocols. It's the foundation for client-server communication.",
        "testStrategy": "Unit tests for connection establishment, disconnection, and basic reconnection. Integration tests to simulate network interruptions and verify the client's ability to re-establish connections and resume communication.",
        "priority": "high",
        "dependencies": [
          3,
          4
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Build Public Client API Wrappers for CRUD & Basic Queries",
        "description": "Create the user-facing, strongly-typed Client API for interacting with the database, supporting CRUD operations and basic queries (`query`, `queryOne`).",
        "details": "This task involves exposing the low-level in-memory CRUD and query engine functionalities through a developer-friendly, type-safe API. It should provide methods like `client.database.create()`, `update()`, `replace()`, `delete()`, `query()`, and `queryOne()` as defined in the PRD.",
        "testStrategy": "Integration tests covering all exposed API methods, ensuring they correctly interact with the underlying in-memory store and return expected results. Verify type safety with TypeScript compilation checks.",
        "priority": "high",
        "dependencies": [
          3,
          4
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Client-side Reactive Subscriptions",
        "description": "Integrate reactive subscriptions (`subscribe`, `subscribeOne`) on the client, allowing applications to react to local data changes in real-time.",
        "details": "This feature leverages the internal change tracking mechanism of the triple store to notify subscribers when relevant data changes. It should support subscribing to query results and single documents.",
        "testStrategy": "Unit tests for subscription setup and teardown. Integration tests to verify that subscribers receive correct updates when data is created, updated, or deleted locally. Test for filtering and query-specific subscriptions.",
        "priority": "high",
        "dependencies": [
          6
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement Basic Client-side Real-time Rooms Functionality",
        "description": "Develop the client-side API and logic for basic Real-time Room features: `join`, `emit` events, `setUserStatus`, `on` event listeners, `onUserStatus`, and `onRoomStatus`.",
        "details": "This task focuses on the client's ability to interact with the room system, send and receive ephemeral events, and manage user presence/status within a room. It assumes a server-side counterpart will handle the actual room state.",
        "testStrategy": "Unit tests for client-side API methods. Integration tests with a mock server to verify event emission, status updates, and listener callbacks function correctly.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Develop Server-side Triple Store & Query Engine (SQLite/Turso)",
        "description": "Implement the server-side triple store, designed to be schema-agnostic, and its query engine, utilizing SQLite/Turso for persistence.",
        "details": "This involves setting up the database schema for storing triples (subject, predicate, object, HLC timestamp) and developing the server-side query capabilities. It should be optimized for multi-tenant access and efficient data retrieval.",
        "testStrategy": "Unit tests for database schema, data insertion, and basic query operations. Performance tests for data retrieval on large datasets. Integration tests to ensure data consistency and integrity.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Implement Server Sync Protocol Handler & HLC Conflict Resolution",
        "description": "Develop the server-side Sync Engine, including the sync protocol handler, multi-tenant coordination, HLC conflict resolution, and change broadcasting.",
        "details": "This is the core server component for real-time data synchronization. It needs to receive client changes, apply HLC-based conflict resolution (field-level, last-write-wins), persist changes, and broadcast updates to other connected clients. It must handle multi-tenancy securely.",
        "testStrategy": "Integration tests simulating multiple clients syncing concurrently, verifying conflict resolution and data consistency. Performance tests for sync throughput. Security tests for tenant isolation.",
        "priority": "high",
        "dependencies": [
          5,
          9
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "Implement Server-side Real-time Rooms (In-memory)",
        "description": "Develop the in-memory server-side component for Real-time Rooms, managing ephemeral pub/sub, per-user session status, and room-wide shared status.",
        "details": "This component will handle the logic for joining/leaving rooms, broadcasting events to room members, maintaining user presence, and managing shared room state. It's designed for ephemeral data like cursors or typing indicators.",
        "testStrategy": "Unit tests for room management logic (join, leave, emit). Integration tests with multiple client connections to verify real-time event broadcasting and status updates within rooms. Load tests for concurrent room activity.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Implement Client-side Batch Operations (`batch.execute`)",
        "description": "Develop the `client.database.batch.execute()` method for performing non-atomic bulk write operations (e.g., `updateMany`, `replaceMany`, `deleteMany`).",
        "details": "This API should allow developers to group multiple write operations into a single request, improving efficiency for scenarios involving many changes. It should handle the internal processing and replication of these bulk changes.",
        "testStrategy": "Unit tests for batch execution logic. Integration tests to verify that bulk operations correctly modify data and are replicated. Performance tests for large batch sizes.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Develop Client-side Pagination (`paginated`, `preloadPaginated`)",
        "description": "Implement efficient pagination capabilities for large datasets on the client, including `.paginated()` and `.preloadPaginated()` methods.",
        "details": "This feature allows fetching and displaying data in chunks, optimizing performance and user experience for large result sets. `preloadPaginated` should enable pre-fetching subsequent pages.",
        "testStrategy": "Unit tests for pagination logic (offset, limit, next/previous page). Integration tests to verify correct data retrieval and display across multiple pages. Performance tests for paginated queries.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Implement Client-side Preloading (`preload`, `preloadOne`)",
        "description": "Extend the client API with `preload` and `preloadOne` methods for server-side data fetching, aiming to eliminate loading states in UI frameworks.",
        "details": "These methods should allow applications to proactively fetch data from the server before it's needed by the UI, enabling seamless transitions and server-side rendering scenarios. This requires coordination with the server-side query engine.",
        "testStrategy": "Integration tests to verify data is preloaded correctly and available instantly upon UI rendering. Performance tests to measure the impact on initial load times.",
        "priority": "medium",
        "dependencies": [
          6,
          10
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Add Advanced Query Features (Aggregations, `groupBy`)",
        "description": "Implement advanced query capabilities such as aggregations (`sum`, `avg`, `count`, `min`, `max`) and `groupBy` clauses within the client-side query engine.",
        "details": "This enhances the analytical power of the query engine, allowing developers to perform more complex data analysis directly on the client's local data. It will require extending the internal query processing logic.",
        "testStrategy": "Unit tests for each aggregation function and `groupBy` logic. Integration tests to verify complex queries combining aggregations and grouping return accurate results. Performance tests for these advanced queries.",
        "priority": "medium",
        "dependencies": [
          4,
          6
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Integrate Client-side IndexedDB Persistence",
        "description": "Implement the IndexedDB Adapter for the client-side persistence layer, enabling full offline capability and faster local startup times.",
        "details": "This involves replacing or augmenting the in-memory triple store with a persistent IndexedDB backend. Changes should be written to IndexedDB, and data should be loaded from it on startup. This is crucial for robust offline functionality.",
        "testStrategy": "Integration tests to verify data persistence across browser sessions. Offline tests to ensure full application functionality without network connectivity. Performance tests for IndexedDB read/write operations.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Enhance Network Resilience",
        "description": "Improve the sync engine's network resilience by implementing advanced connection recovery, partition handling, and automatic reconnection with timestamp-based gap recovery.",
        "details": "This task focuses on making the system robust against flaky networks. It includes sophisticated retry mechanisms, detecting and resolving data inconsistencies after network partitions, and ensuring seamless re-synchronization upon reconnection using HLC timestamps.",
        "testStrategy": "Stress tests simulating network drops, high latency, and intermittent connectivity. Integration tests to verify automatic reconnection and data consistency after network failures. Concurrency tests during network partitions.",
        "priority": "medium",
        "dependencies": [
          5,
          10
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Implement Authentication & Authorization",
        "description": "Develop robust client authentication (e.g., JWT-based) and server-side row-level security, potentially including a management dashboard for App ID generation.",
        "details": "This involves securing client-server communication, verifying user identities, and enforcing fine-grained access control to data based on user roles and permissions. The management dashboard would facilitate app and user management.",
        "testStrategy": "Security tests for authentication flows (login, token refresh). Integration tests to verify row-level security rules are correctly enforced. Penetration testing for common vulnerabilities.",
        "priority": "medium",
        "dependencies": [
          10
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Implement Storage Optimization (Pruning, Compaction, Caching)",
        "description": "Develop strategies for data pruning, compaction, and advanced caching for both client and server triple stores to optimize storage and performance.",
        "details": "This includes mechanisms to remove old or irrelevant data (pruning), consolidate storage (compaction), and implement multi-level caching strategies to reduce database load and improve query response times.",
        "testStrategy": "Performance tests measuring storage footprint and query times before/after optimization. Stress tests to verify caching effectiveness under high load. Data integrity tests after pruning/compaction.",
        "priority": "low",
        "dependencies": [
          9,
          16
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Develop Framework Integrations (Hooks/Wrappers)",
        "description": "Provide official hooks and wrappers for popular UI frameworks beyond React and Svelte (e.g., Vue, Angular) to simplify integration.",
        "details": "This involves creating framework-specific libraries that abstract away the direct Client API calls, providing idiomatic ways to interact with the system (e.g., custom hooks for data fetching, components for real-time updates).",
        "testStrategy": "Integration tests for each framework-specific wrapper, ensuring correct data flow and reactivity. Develop example applications for each integrated framework. Gather developer feedback.",
        "priority": "low",
        "dependencies": [
          6,
          7
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 21,
        "title": "Implement Advanced Filtering/Query Engine",
        "description": "Implement an advanced filtering/query engine that supports complex `where` clauses with multiple operators (e.g., `equals`, `greaterThan`, `lessThan`, etc.) for use in `query`, `updateMany`, `replaceMany`, and `deleteMany` methods.",
        "details": "",
        "testStrategy": "",
        "status": "cancelled",
        "dependencies": [],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 22,
        "title": "Implement `createdBy` field in entities",
        "description": "Add logic to automatically include a `createdBy` field on document creation, storing the ID of the user who created it. This will be blocked by a future authentication implementation.",
        "details": "",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 23,
        "title": "Implement Datalog Query Engine for TripleStore",
        "description": "Develop a Datalog-style query engine within the TripleStore to enable complex, declarative query capabilities directly on the server, offloading logic from the client.",
        "details": "This task involves a significant enhancement to the server-side TripleStore. Key steps include:\n\n1.  **Datalog Language Definition**: Define a precise subset of Datalog (or a Datalog-like language) that aligns with the TripleStore's data model. This should include support for basic predicates, rules, recursion, and potentially stratified negation or aggregation.\n2.  **Parser Implementation**: Develop a robust parser to convert Datalog query strings and rule definitions into an internal Abstract Syntax Tree (AST) representation.\n3.  **Query Optimizer Design**: Implement a query optimizer responsible for transforming the AST into an efficient execution plan. This may involve techniques like goal reordering, join optimization, and index utilization strategies.\n4.  **Execution Engine Development**: Build the core Datalog evaluation engine. This will likely involve implementing algorithms such as semi-naive evaluation for recursive rules, or other established Datalog evaluation techniques. The engine must efficiently interact with the underlying TripleStore data structures to retrieve and process triples.\n5.  **TripleStore Integration Layer**: Create a dedicated API or module within the TripleStore that allows the Datalog engine to seamlessly access, query, and potentially update (if write capabilities are included) the stored triples.\n6.  **Client-Server Interface**: Define and implement a new server-side endpoint (e.g., via HTTP POST or WebSocket messages) specifically for receiving Datalog queries from clients and returning structured results. This will require careful consideration of serialization formats for queries and results.\n7.  **Error Handling and Debugging**: Implement comprehensive error reporting for syntax errors, logical inconsistencies in rules, and runtime execution failures. Provide logging and potential tracing mechanisms to aid in debugging complex Datalog programs.",
        "testStrategy": "A multi-faceted testing approach is crucial for this complex component:\n\n*   **Unit Tests**: Develop extensive unit tests for each sub-component:\n    *   **Parser**: Test with a wide range of valid and invalid Datalog syntax, ensuring correct AST generation and error handling.\n    *   **Optimizer**: Test specific optimization rules (e.g., join reordering, predicate pushdown) with small, controlled query plans.\n    *   **Execution Engine**: Test core evaluation logic with small datasets and simple rules (non-recursive, recursive, with negation) to verify correct inference and query results.\n*   **Integration Tests**: \n    *   Set up a dedicated test TripleStore instance populated with diverse sample data.\n    *   Execute complex Datalog queries and rule sets against this instance, verifying the results against known expected outcomes.\n    *   Test the end-to-end client-server interface by sending Datalog queries from a mock client and asserting the correctness and format of the received responses.\n    *   Verify the engine's ability to handle various data types and edge cases present in the TripleStore.\n*   **Performance Tests**: \n    *   Conduct performance benchmarks with large datasets and increasingly complex/recursive queries to identify bottlenecks and ensure acceptable query execution times.\n    *   Evaluate the engine's scalability under concurrent query loads.\n*   **Regression Tests**: Ensure that the introduction of the Datalog engine does not negatively impact the performance or stability of existing TripleStore functionalities.",
        "status": "done",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Datalog Query Structure",
            "description": "Define the structure for Datalog-like queries that the `TripleStore` will accept, including clauses, variables, and rules.",
            "dependencies": [],
            "details": "Define a precise subset of Datalog (or a Datalog-like language) that aligns with the TripleStore's data model. This should include support for basic predicates, rules, recursion, and potentially stratified negation or aggregation. This forms the foundational specification for the Datalog language to be implemented.",
            "status": "done",
            "testStrategy": "The correctness and completeness of this design will be implicitly validated through the successful implementation and testing of subsequent parser and engine components."
          },
          {
            "id": 2,
            "title": "Implement Datalog Query Parser in TripleStore",
            "description": "Add a method to `TripleStore` to parse these Datalog queries.",
            "dependencies": [
              "23.1"
            ],
            "details": "Develop a robust parser to convert Datalog query strings and rule definitions into an internal Abstract Syntax Tree (AST) representation, based on the Datalog structure defined in subtask 23.1. This parser must handle syntax validation and provide clear error reporting for malformed queries.",
            "status": "done",
            "testStrategy": "Develop extensive unit tests for the parser: Test with a wide range of valid and invalid Datalog syntax, ensuring correct AST generation and comprehensive error handling for various syntax errors."
          },
          {
            "id": 3,
            "title": "Implement Datalog Query Engine in TripleStore",
            "description": "Build the core logic to execute parsed Datalog queries against the stored triples, including unification and finding variable bindings.",
            "dependencies": [
              "23.1",
              "23.2"
            ],
            "details": "Build the core Datalog evaluation engine. This will likely involve implementing algorithms such as semi-naive evaluation for recursive rules, or other established Datalog evaluation techniques. It must efficiently interact with the underlying TripleStore data structures to retrieve and process triples, including unification and finding variable bindings. This subtask also encompasses the design and implementation of a query optimizer responsible for transforming the AST into an efficient execution plan, potentially involving techniques like goal reordering, join optimization, and index utilization strategies.\n<info added on 2025-09-04T19:22:55.268Z>\nSpecifically, multiple attempts to implement the Datalog join algorithm have failed due to persistent issues in correctly handling the unification of clauses with existing variable bindings. This indicates a fundamental misunderstanding of the required logic for variable binding propagation and unification during joins. A complete re-evaluation and redesign of the Datalog join algorithm, focusing on robust and correct unification, is required before proceeding.\n</info added on 2025-09-04T19:22:55.268Z>",
            "status": "done",
            "testStrategy": "Develop extensive unit tests for the core evaluation algorithms and query optimizer. Integration tests to verify correct execution of complex Datalog queries, including recursive rules, against the TripleStore, ensuring performance, correctness, and proper handling of variable bindings and joins."
          },
          {
            "id": 4,
            "title": "Expose Datalog Query Method from TripleStore",
            "description": "Create a public `datalogQuery` method on `TripleStore`.",
            "dependencies": [
              "23.3"
            ],
            "details": "Create a dedicated API or module within the TripleStore that allows the Datalog engine to seamlessly access, query, and potentially update (if write capabilities are included) the stored triples. Define and implement a new server-side endpoint (e.g., via HTTP POST or WebSocket messages) specifically for receiving Datalog queries from clients and returning structured results. This will require careful consideration of serialization formats for queries and results.",
            "status": "done",
            "testStrategy": "Integration tests to verify the server-side endpoint correctly receives Datalog queries, invokes the Datalog engine, and returns structured results in the defined format. Test error handling for invalid requests and malformed Datalog queries received via the endpoint."
          },
          {
            "id": 5,
            "title": "Refactor client.ts to use new TripleStore query method",
            "description": "Update the `query` implementation in `client.ts` to use the new, more powerful query capabilities of the `TripleStore`, simplifying the client-side logic.",
            "dependencies": [
              "23.4"
            ],
            "details": "This is a client-side task. It involves modifying the existing client-side `query` logic to leverage the new server-side Datalog engine. This will likely involve sending Datalog query strings to the server and processing the structured results received back, thereby offloading complex query logic from the client and simplifying its implementation.",
            "status": "done",
            "testStrategy": "End-to-end integration tests to ensure the client-side `query` method correctly interacts with the new server-side Datalog engine, retrieves expected data, and that the client-side logic is indeed simplified as intended. Test various query scenarios from the client perspective."
          },
          {
            "id": 6,
            "title": "Implement Optional Clauses in Datalog Engine",
            "description": "Implement support for optional clauses in the Datalog engine. This will allow queries to match entities that may not have a specific triple, such as the `_deleted` flag.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 23
          },
          {
            "id": 7,
            "title": "Implement LWW in Datalog Engine",
            "description": "Modify the Datalog engine to handle last-write-wins (LWW) semantics by incorporating HLC timestamps into the query logic. This will ensure that only the latest value for each field is returned.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 23
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-29T02:04:22.495Z",
      "updated": "2025-09-04T20:32:58.248Z",
      "description": "Tasks for master context"
    }
  }
}