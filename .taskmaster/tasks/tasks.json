{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Design Core Triple Store & HLC Mechanism",
        "description": "Define the fundamental data model for the triple store (subject, predicate, object) and the structure/logic for Hybrid Logical Clocks (HLC) at the field level.",
        "details": "This task involves designing the in-memory representation of the triple store, including indexing strategies (e.g., by subject, predicate, object), and the HLC timestamp format (e.g., (logical_time, wall_clock_time, node_id)). It also covers the basic principles of HLC generation and comparison for conflict resolution.",
        "testStrategy": "Document design specifications. Conduct design reviews with relevant stakeholders. Create mock data structures to validate the model's flexibility and efficiency for HLC integration.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Core Triple Data Model Structure",
            "description": "Specify the fundamental structure of a triple (subject, predicate, object) and the allowed internal data types for each component.",
            "dependencies": [],
            "details": "This involves defining the internal representation of subjects, predicates, and objects, considering potential UUIDs, strings, or other identifiers for efficient storage and retrieval. It also covers the conceptual schema for a triple.\n<info added on 2025-08-29T02:16:10.669Z>\nInitial core data model types (`Triple`, `Subject`, `Predicate`, `Object`, `Value`, `Ref`, `RefMany`, and a placeholder for `HLC` as a string) have been defined in `index.ts`.\n</info added on 2025-08-29T02:16:10.669Z>",
            "status": "done",
            "testStrategy": "Document the data model specification. Conduct design reviews with data architects and system designers."
          },
          {
            "id": 2,
            "title": "Design In-Memory Triple Store Representation",
            "description": "Design the optimal in-memory data structures for storing triples efficiently, prioritizing fast read and write operations.",
            "dependencies": [
              "1.1"
            ],
            "details": "This involves selecting appropriate data structures (e.g., nested hash maps, arrays, custom structures) to hold the triples, considering memory footprint and access patterns.\n<info added on 2025-08-29T02:26:34.861Z>\nThe in-memory TripleStore has been implemented using a class-based approach, incorporating SPO, POS, and OSP indexes to facilitate efficient querying. It supports adding, removing, and querying triples, including those with partial patterns.\n</info added on 2025-08-29T02:26:34.861Z>",
            "status": "done",
            "testStrategy": "Document the in-memory structure design. Create mock data structures to validate memory efficiency and basic access patterns. Conduct design reviews."
          },
          {
            "id": 3,
            "title": "Design Subject-Predicate-Object Indexing Strategy",
            "description": "Define the indexing strategies for efficient retrieval of triples based on subject, predicate, and object components.",
            "dependencies": [
              "1.2"
            ],
            "details": "This includes designing specific index structures (e.g., SPO, POS, OSP permutations) to support various query patterns and ensure rapid lookup times.\n<info added on 2025-08-29T02:26:45.480Z>\nImplemented SPO, POS, and OSP indexing within the TripleStore class to allow for efficient querying of triples by any combination of subject, predicate, and object.\n</info added on 2025-08-29T02:26:45.480Z>",
            "status": "done",
            "testStrategy": "Document the indexing strategy. Simulate common query patterns against proposed index structures to estimate performance. Conduct design reviews."
          },
          {
            "id": 4,
            "title": "Define Hybrid Logical Clock (HLC) Timestamp Format",
            "description": "Specify the exact structure and components of the HLC timestamp, including logical time, wall clock time, and node identifier.",
            "dependencies": [],
            "details": "This involves defining the data types, precision, and ranges for each component of the HLC timestamp (e.g., 64-bit integers for logical and wall clock time, a unique identifier for node ID).\n<info added on 2025-08-29T02:19:21.933Z>\nImplemented HLC creation, parsing, incrementing, and comparison functions in `index.ts`. Added a full test suite in `index.test.ts` using `bun:test` and all tests are passing.\n</info added on 2025-08-29T02:19:21.933Z>\n<info added on 2025-08-29T02:25:10.622Z>\n<info added on 2024-07-30T12:00:00.000Z>\nRefactored the HLC implementation from a string-based approach to a class-based, object-oriented design. The HLC class now encapsulates all related logic, improving maintainability and type safety.\n</info added on 2024-07-30T12:00:00.000Z>\n</info added on 2025-08-29T02:25:10.622Z>",
            "status": "pending",
            "testStrategy": "Document the HLC timestamp format specification. Conduct design reviews to ensure compatibility and uniqueness requirements are met."
          },
          {
            "id": 5,
            "title": "Design HLC Generation Logic",
            "description": "Define the algorithm for generating new HLC timestamps for local operations and when merging with remote timestamps.",
            "dependencies": [
              "1.4"
            ],
            "details": "This includes specifying rules for incrementing the logical time component, updating the wall clock time component, and handling potential clock skew during generation and merge operations.",
            "status": "done",
            "testStrategy": "Document the HLC generation algorithm. Create pseudocode or flowcharts to illustrate the logic. Conduct design reviews."
          },
          {
            "id": 6,
            "title": "Design HLC Comparison Logic",
            "description": "Define the rules and algorithm for comparing two HLC timestamps to determine causality or precedence.",
            "dependencies": [
              "1.4"
            ],
            "details": "This involves specifying how the logical time, wall clock time, and node ID components are used in the comparison process to establish a total or partial order.",
            "status": "done",
            "testStrategy": "Document the HLC comparison algorithm. Define edge cases for comparison and verify logic. Conduct design reviews."
          },
          {
            "id": 7,
            "title": "Design Field-Level HLC Application within Triples",
            "description": "Specify precisely how HLC timestamps are associated with individual fields (objects) within the triple store for fine-grained versioning and conflict resolution.",
            "dependencies": [
              "1.1",
              "1.4"
            ],
            "details": "This involves deciding whether the HLC is embedded within the object value, stored as a separate metadata field alongside each triple, or managed through a versioning layer.\n<info added on 2025-08-29T02:44:45.628Z>\nIt has been decided that the HLC will be stored as a fourth element within each triple, forming `[Subject, Predicate, Object, HLC]`. This structure inherently supports field-level HLC application, where each triple's HLC timestamps the specific field (predicate) for a given subject. This design satisfies the requirement for fine-grained versioning and conflict resolution at the field level, and no further dedicated implementation is required for this aspect.\n</info added on 2025-08-29T02:44:45.628Z>",
            "status": "done",
            "testStrategy": "Document the HLC application strategy. Create mock data structures to illustrate HLC placement and association. Conduct design reviews."
          },
          {
            "id": 8,
            "title": "Design HLC-based Conflict Resolution Strategy",
            "description": "Define the primary conflict resolution strategy using HLCs, specifically focusing on 'last-write-wins' at the field level.",
            "dependencies": [
              "1.6",
              "1.7"
            ],
            "details": "This involves outlining the precise logic for selecting the 'winning' triple based on HLC comparison when concurrent updates to the same field occur, ensuring deterministic outcomes.\n<info added on 2025-08-29T02:48:19.440Z>\nWhen adding a triple, if a triple with the same subject, predicate, and object already exists, the HLCs are compared, and only the triple with the highest HLC is retained. This ensures data consistency in the face of concurrent updates.\n</info added on 2025-08-29T02:48:19.440Z>",
            "status": "done",
            "testStrategy": "Document the conflict resolution algorithm. Define various conflict scenarios and demonstrate how the HLC-based strategy resolves them. Conduct design reviews."
          },
          {
            "id": 9,
            "title": "Design for Triple Store Mutability and Versioning",
            "description": "Determine how updates to triples are handled, considering whether triples are immutable (new version created) or mutable (in-place update), and how HLCs integrate with this.",
            "dependencies": [
              "1.2",
              "1.7"
            ],
            "details": "This impacts how historical versions might be retained or discarded based on HLCs and the overall data lifecycle within the in-memory store.\n<info added on 2025-08-29T02:54:39.255Z>\nThe store is designed as an immutable, versioned TripleStore. For any given subject and predicate, only the triple with the highest HLC timestamp is retained, implementing a 'last-write-wins' strategy to ensure the store always reflects the latest version of a fact.\n</info added on 2025-08-29T02:54:39.255Z>",
            "status": "done",
            "testStrategy": "Document the mutability/versioning model. Illustrate update flows with HLCs. Conduct design reviews."
          },
          {
            "id": 10,
            "title": "Design for Data Type Representation within Triples",
            "description": "Define how various client-side data types (string, number, boolean, date, ref, refMany) are represented and stored as objects within the triple store.",
            "dependencies": [
              "1.1"
            ],
            "details": "This includes considerations for serialization/deserialization, efficient storage, and type integrity for different data types when they form the object component of a triple.\n<info added on 2025-08-29T02:38:57.090Z>\nA prefix-based encoding system has been designed and implemented in `crdt.ts` to represent various data types (strings, numbers, booleans, dates, and null values) for the object component of triples. This system is fully tested.\n</info added on 2025-08-29T02:38:57.090Z>\n<info added on 2025-08-29T02:44:01.399Z>\n<info added on 2025-08-29T02:38:57.090Z>\nPer user feedback, the decision has been made to store primitive data types directly in memory without an encoding layer. The previously designed and implemented prefix-based encoding system in `crdt.ts` is being removed.\n</info added on 2025-08-29T02:38:57.090Z>\n</info added on 2025-08-29T02:44:01.399Z>",
            "status": "done",
            "testStrategy": "Document the data type representation mapping. Create examples of how different types are stored. Conduct design reviews."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Client-side Schema Definition (`createSchema`)",
        "description": "Develop the `createSchema` utility for client-only schema definition, enabling type safety and schema evolution through field-level storage and fallbacks.",
        "details": "The utility should support defining entities and rooms with specific field types (t.string, t.number, t.boolean, t.date, t.ref, t.refMany). It needs to parse the schema, generate internal type definitions, and provide mechanisms for schema validation during local operations.",
        "testStrategy": "Unit tests for `createSchema` parsing and validation logic. Integration tests to ensure defined schemas correctly influence data type handling and error reporting for invalid data.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement `createSchema` Core & Type Parsing",
            "description": "Develop the foundational `createSchema` utility, enabling the definition of entities and rooms. This subtask includes parsing the input schema definition (e.g., `t.string`, `t.number`, `t.ref`) and converting it into a robust internal data structure suitable for type checking and further processing.",
            "dependencies": [],
            "details": "Define the internal representation for schema types. Implement the parsing logic within `createSchema` to transform user-defined schema objects into this internal format. Ensure support for `t.string`, `t.number`, `t.boolean`, `t.date`, `t.ref`, `t.refMany`.\n<info added on 2025-08-29T03:00:04.746Z>\nThe internal, optimized Map representation of schema definitions, established by the `createSchema` utility, is now available within the `Schema` object to serve as a strong foundation for efficient validation and type-checking.\n</info added on 2025-08-29T03:00:04.746Z>\n<info added on 2025-08-29T03:04:34.497Z>\nThe schema definition now includes support for `fallback` and `optional` field options, and the internal representation generated by `createSchema` fully encompasses both `entities` and `rooms` structures, which the validation logic must now robustly handle.\n</info added on 2025-08-29T03:04:34.497Z>",
            "status": "done",
            "testStrategy": "Unit tests for `createSchema` input parsing, ensuring correct internal representation for various valid and invalid schema definitions."
          },
          {
            "id": 2,
            "title": "Develop Schema Validation Logic",
            "description": "Implement the core validation engine that uses the internally defined schema to enforce type safety during local data operations (e.g., create, update). This mechanism will ensure that data conforms to the defined types and structures.",
            "dependencies": [
              "2.1"
            ],
            "details": "Design and implement validation functions that can be invoked when data is written or updated. These functions should check field types, required fields, and reference integrity based on the parsed schema.\n<info added on 2025-08-29T03:15:38.278Z>\nImplemented the `Schema.validate` method for robust runtime validation of objects against the schema. This includes checks for correct field types, required fields, and the proper structure of `ref` and `refMany` reference types, ensuring data integrity. The functionality is fully tested.\n</info added on 2025-08-29T03:15:38.278Z>",
            "status": "done",
            "testStrategy": "Unit tests for validation rules against various data inputs (valid, invalid types, missing required fields, incorrect references). Integration tests to confirm validation errors are correctly reported during mock data operations."
          },
          {
            "id": 3,
            "title": "Design & Implement Schema Versioning",
            "description": "Develop a mechanism to support schema evolution, including versioning of schemas and handling field-level storage. This allows for backward and forward compatibility as the application's data model changes over time.",
            "dependencies": [
              "2.1"
            ],
            "details": "Define how schema versions will be managed and associated with data. Implement logic to store schema version metadata alongside data or within the schema definition itself. Consider strategies for migrating data between schema versions if necessary.",
            "status": "done",
            "testStrategy": "Unit tests for schema version assignment and retrieval. Integration tests to verify that data can be associated with specific schema versions."
          },
          {
            "id": 4,
            "title": "Implement Field-Level Fallbacks",
            "description": "Add support for defining and applying field-level fallbacks within the schema. This feature provides default values or alternative logic when a field is missing or invalid, enhancing data robustness and schema evolution.",
            "dependencies": [
              "2.1"
            ],
            "details": "Extend the schema definition to allow specifying fallback values or functions for individual fields. Implement the runtime logic to apply these fallbacks during data retrieval or validation if a field's value is not present or does not conform.",
            "status": "done",
            "testStrategy": "Unit tests for various fallback scenarios (missing fields, invalid types triggering fallback). Integration tests to ensure fallbacks are correctly applied when querying data."
          },
          {
            "id": 5,
            "title": "Document `createSchema` Utility & Examples",
            "description": "Create comprehensive developer documentation for the `createSchema` utility, covering its API, usage patterns, best practices, and detailed examples for defining entities, rooms, and various field types.",
            "dependencies": [
              "2.1",
              "2.2",
              "2.3",
              "2.4"
            ],
            "details": "Write API reference documentation for `createSchema` and its associated type helpers (`t.string`, `t.ref`, etc.). Include examples demonstrating schema definition, validation implications, and how to leverage versioning and fallbacks.",
            "status": "done",
            "testStrategy": "Review documentation for clarity, completeness, and accuracy, ensuring all features are adequately explained with practical examples."
          }
        ]
      },
      {
        "id": 3,
        "title": "Develop Low-level Client-side In-memory CRUD Operations",
        "description": "Implement the core create, read, update, and delete operations directly on the client's in-memory triple store, incorporating HLC for local conflict resolution.",
        "details": "This involves the internal logic for manipulating triples, generating HLC timestamps for new or updated fields, and applying last-write-wins conflict resolution for same-field concurrent updates locally. Operations should be instant and provide immediate feedback.",
        "testStrategy": "Unit tests for each CRUD operation (create, update, delete, read). Concurrency tests to simulate local concurrent writes to the same field and verify HLC-based last-write-wins behavior. Performance tests for basic operations on a small dataset.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "todo",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Client Factory & In-memory Triple Store",
            "description": "Set up the client factory, initialize the in-memory triple store, and integrate the HLC mechanism for timestamp generation and local conflict resolution. This forms the foundational layer for all subsequent CRUD operations.",
            "dependencies": [],
            "details": "Implement the `createClient` factory function. Establish the internal data structure for the in-memory triple store (e.g., `Map<subject, Map<predicate, {value, hlc_timestamp}>>`). Integrate the HLC utility (from Task 1) for generating timestamps for new/updated triples. Define the last-write-wins conflict resolution logic for same-field concurrent updates.",
            "status": "pending",
            "testStrategy": "Unit tests for client factory instantiation, in-memory store initialization, and basic HLC timestamp generation and application during initial data insertion."
          },
          {
            "id": 2,
            "title": "Implement `create` Operation with Schema Validation & Fallbacks",
            "description": "Develop the `create` method for adding new entities to the in-memory store, ensuring schema validation and applying fallback values as defined in the schema.",
            "dependencies": [
              "3.1"
            ],
            "details": "Implement the `client.create()` method. Integrate with the schema definition utility (from Task 2) to validate incoming data against the defined schema. Apply fallback values for missing fields as per schema. Generate HLC timestamps for all new triples using the integrated HLC utility.",
            "status": "pending",
            "testStrategy": "Unit tests for successful creation, schema validation errors, fallback application, and correct HLC timestamping."
          },
          {
            "id": 3,
            "title": "Implement `read` Operation for Entity Retrieval",
            "description": "Develop the `read` method to efficiently retrieve entities or specific fields from the client's in-memory triple store.",
            "dependencies": [
              "3.1"
            ],
            "details": "Implement `client.read()` to fetch entities by ID or query for specific fields. Ensure efficient retrieval from the in-memory store. Consider supporting basic filtering or projection if implied by `client-api.md`.",
            "status": "pending",
            "testStrategy": "Unit tests for retrieving existing entities, non-existent entities, specific fields, and handling various query parameters."
          },
          {
            "id": 4,
            "title": "Implement `update` & `replace` Operations with Functional Updates",
            "description": "Develop the `update` and `replace` methods to modify existing entities, supporting functional updates and applying HLC-based last-write-wins conflict resolution.",
            "dependencies": [
              "3.1",
              "3.3"
            ],
            "details": "Implement `client.update()` and `client.replace()`. Support passing functions to update fields (e.g., `count: c => c + 1`). Generate new HLC timestamps for updated fields using the integrated HLC utility. Apply last-write-wins logic based on HLC timestamps for concurrent updates to the same field.",
            "status": "pending",
            "testStrategy": "Unit tests for direct updates, functional updates, `replace` behavior, and conflict resolution scenarios (last-write-wins verification)."
          },
          {
            "id": 5,
            "title": "Implement `delete` Operation for Entities/Fields",
            "description": "Develop the `delete` method to remove entities or specific fields from the in-memory triple store.",
            "dependencies": [
              "3.1"
            ],
            "details": "Implement `client.delete()` to remove entire entities by ID or specific predicates for an entity. Ensure immediate removal from the in-memory store.",
            "status": "pending",
            "testStrategy": "Unit tests for deleting existing entities, non-existent entities, specific fields, and verifying removal."
          },
          {
            "id": 6,
            "title": "Implement Bulk `updateMany` & `replaceMany` Operations",
            "description": "Develop bulk `updateMany` and `replaceMany` methods for efficient modification of multiple entities, leveraging the single update/replace logic.",
            "dependencies": [
              "3.4"
            ],
            "details": "Implement `client.updateMany()` and `client.replaceMany()`. These methods should iterate over the provided entities/updates and call the respective single `update` or `replace` logic internally, ensuring HLC and conflict resolution are applied per entity/field. Optimize for batch processing where possible.",
            "status": "pending",
            "testStrategy": "Unit tests for bulk updates/replaces on multiple entities, including mixed success/failure scenarios and performance considerations for large batches."
          },
          {
            "id": 7,
            "title": "Implement Bulk `deleteMany` Operation",
            "description": "Develop the bulk `deleteMany` method for efficient removal of multiple entities or fields from the in-memory store.",
            "dependencies": [
              "3.5"
            ],
            "details": "Implement `client.deleteMany()`. This method should iterate over the provided entity IDs/field paths and call the single `delete` logic internally. Optimize for batch processing.",
            "status": "pending",
            "testStrategy": "Unit tests for bulk deletes on multiple entities/fields, including partial success and performance."
          },
          {
            "id": 8,
            "title": "Comprehensive CRUD Testing & Performance Benchmarking",
            "description": "Conduct comprehensive unit, integration, and performance tests for all implemented in-memory CRUD operations, including HLC-based conflict resolution.",
            "dependencies": [
              "3.2",
              "3.3",
              "3.4",
              "3.5",
              "3.6",
              "3.7"
            ],
            "details": "Write extensive unit tests covering edge cases for each CRUD method. Develop integration tests to verify end-to-end data flow and consistency across multiple operations. Implement concurrency tests to simulate local concurrent writes to the same field and verify HLC-based last-write-wins behavior. Conduct performance benchmarks for basic operations on varying dataset sizes to ensure 'instant' feedback.",
            "status": "pending",
            "testStrategy": "As described in the details. Focus on robustness, correctness, and performance."
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement Client-side In-memory Query Engine (Basic)",
        "description": "Develop the client-side query engine to support fundamental query capabilities on the in-memory triple store, including `fields`, `where`, `orderBy`, and `limit` operators.",
        "details": "The query engine should efficiently retrieve data based on specified criteria. It needs to handle filtering by field values, selecting specific fields, ordering results, and limiting the number of returned documents. This is the internal query mechanism.",
        "testStrategy": "Unit tests for each query operator (`where`, `orderBy`, `limit`, `fields`). Integration tests to verify complex queries combining multiple operators return correct results from the in-memory store. Performance tests for queries on medium-sized datasets.",
        "priority": "high",
        "dependencies": [
          1,
          3
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Establish Client-side WebSocket & HTTP Connection Management",
        "description": "Set up robust client-side connection management for WebSocket (real-time updates) and HTTP (bulk operations, initial data loading) for replication.",
        "details": "This includes establishing and maintaining connections, handling disconnections, basic reconnection logic, and managing data flow over these protocols. It's the foundation for client-server communication.",
        "testStrategy": "Unit tests for connection establishment, disconnection, and basic reconnection. Integration tests to simulate network interruptions and verify the client's ability to re-establish connections and resume communication.",
        "priority": "high",
        "dependencies": [
          3,
          4
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Build Public Client API Wrappers for CRUD & Basic Queries",
        "description": "Create the user-facing, strongly-typed Client API for interacting with the database, supporting CRUD operations and basic queries (`query`, `queryOne`).",
        "details": "This task involves exposing the low-level in-memory CRUD and query engine functionalities through a developer-friendly, type-safe API. It should provide methods like `client.database.create()`, `update()`, `replace()`, `delete()`, `query()`, and `queryOne()` as defined in the PRD.",
        "testStrategy": "Integration tests covering all exposed API methods, ensuring they correctly interact with the underlying in-memory store and return expected results. Verify type safety with TypeScript compilation checks.",
        "priority": "high",
        "dependencies": [
          3,
          4
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Client-side Reactive Subscriptions",
        "description": "Integrate reactive subscriptions (`subscribe`, `subscribeOne`) on the client, allowing applications to react to local data changes in real-time.",
        "details": "This feature leverages the internal change tracking mechanism of the triple store to notify subscribers when relevant data changes. It should support subscribing to query results and single documents.",
        "testStrategy": "Unit tests for subscription setup and teardown. Integration tests to verify that subscribers receive correct updates when data is created, updated, or deleted locally. Test for filtering and query-specific subscriptions.",
        "priority": "high",
        "dependencies": [
          6
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement Basic Client-side Real-time Rooms Functionality",
        "description": "Develop the client-side API and logic for basic Real-time Room features: `join`, `emit` events, `setUserStatus`, `on` event listeners, `onUserStatus`, and `onRoomStatus`.",
        "details": "This task focuses on the client's ability to interact with the room system, send and receive ephemeral events, and manage user presence/status within a room. It assumes a server-side counterpart will handle the actual room state.",
        "testStrategy": "Unit tests for client-side API methods. Integration tests with a mock server to verify event emission, status updates, and listener callbacks function correctly.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Develop Server-side Triple Store & Query Engine (SQLite/Turso)",
        "description": "Implement the server-side triple store, designed to be schema-agnostic, and its query engine, utilizing SQLite/Turso for persistence.",
        "details": "This involves setting up the database schema for storing triples (subject, predicate, object, HLC timestamp) and developing the server-side query capabilities. It should be optimized for multi-tenant access and efficient data retrieval.",
        "testStrategy": "Unit tests for database schema, data insertion, and basic query operations. Performance tests for data retrieval on large datasets. Integration tests to ensure data consistency and integrity.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Implement Server Sync Protocol Handler & HLC Conflict Resolution",
        "description": "Develop the server-side Sync Engine, including the sync protocol handler, multi-tenant coordination, HLC conflict resolution, and change broadcasting.",
        "details": "This is the core server component for real-time data synchronization. It needs to receive client changes, apply HLC-based conflict resolution (field-level, last-write-wins), persist changes, and broadcast updates to other connected clients. It must handle multi-tenancy securely.",
        "testStrategy": "Integration tests simulating multiple clients syncing concurrently, verifying conflict resolution and data consistency. Performance tests for sync throughput. Security tests for tenant isolation.",
        "priority": "high",
        "dependencies": [
          5,
          9
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "Implement Server-side Real-time Rooms (In-memory)",
        "description": "Develop the in-memory server-side component for Real-time Rooms, managing ephemeral pub/sub, per-user session status, and room-wide shared status.",
        "details": "This component will handle the logic for joining/leaving rooms, broadcasting events to room members, maintaining user presence, and managing shared room state. It's designed for ephemeral data like cursors or typing indicators.",
        "testStrategy": "Unit tests for room management logic (join, leave, emit). Integration tests with multiple client connections to verify real-time event broadcasting and status updates within rooms. Load tests for concurrent room activity.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Implement Client-side Batch Operations (`batch.execute`)",
        "description": "Develop the `client.database.batch.execute()` method for performing non-atomic bulk write operations (e.g., `updateMany`, `replaceMany`, `deleteMany`).",
        "details": "This API should allow developers to group multiple write operations into a single request, improving efficiency for scenarios involving many changes. It should handle the internal processing and replication of these bulk changes.",
        "testStrategy": "Unit tests for batch execution logic. Integration tests to verify that bulk operations correctly modify data and are replicated. Performance tests for large batch sizes.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Develop Client-side Pagination (`paginated`, `preloadPaginated`)",
        "description": "Implement efficient pagination capabilities for large datasets on the client, including `.paginated()` and `.preloadPaginated()` methods.",
        "details": "This feature allows fetching and displaying data in chunks, optimizing performance and user experience for large result sets. `preloadPaginated` should enable pre-fetching subsequent pages.",
        "testStrategy": "Unit tests for pagination logic (offset, limit, next/previous page). Integration tests to verify correct data retrieval and display across multiple pages. Performance tests for paginated queries.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Implement Client-side Preloading (`preload`, `preloadOne`)",
        "description": "Extend the client API with `preload` and `preloadOne` methods for server-side data fetching, aiming to eliminate loading states in UI frameworks.",
        "details": "These methods should allow applications to proactively fetch data from the server before it's needed by the UI, enabling seamless transitions and server-side rendering scenarios. This requires coordination with the server-side query engine.",
        "testStrategy": "Integration tests to verify data is preloaded correctly and available instantly upon UI rendering. Performance tests to measure the impact on initial load times.",
        "priority": "medium",
        "dependencies": [
          6,
          10
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Add Advanced Query Features (Aggregations, `groupBy`)",
        "description": "Implement advanced query capabilities such as aggregations (`sum`, `avg`, `count`, `min`, `max`) and `groupBy` clauses within the client-side query engine.",
        "details": "This enhances the analytical power of the query engine, allowing developers to perform more complex data analysis directly on the client's local data. It will require extending the internal query processing logic.",
        "testStrategy": "Unit tests for each aggregation function and `groupBy` logic. Integration tests to verify complex queries combining aggregations and grouping return accurate results. Performance tests for these advanced queries.",
        "priority": "medium",
        "dependencies": [
          4,
          6
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Integrate Client-side IndexedDB Persistence",
        "description": "Implement the IndexedDB Adapter for the client-side persistence layer, enabling full offline capability and faster local startup times.",
        "details": "This involves replacing or augmenting the in-memory triple store with a persistent IndexedDB backend. Changes should be written to IndexedDB, and data should be loaded from it on startup. This is crucial for robust offline functionality.",
        "testStrategy": "Integration tests to verify data persistence across browser sessions. Offline tests to ensure full application functionality without network connectivity. Performance tests for IndexedDB read/write operations.",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Enhance Network Resilience",
        "description": "Improve the sync engine's network resilience by implementing advanced connection recovery, partition handling, and automatic reconnection with timestamp-based gap recovery.",
        "details": "This task focuses on making the system robust against flaky networks. It includes sophisticated retry mechanisms, detecting and resolving data inconsistencies after network partitions, and ensuring seamless re-synchronization upon reconnection using HLC timestamps.",
        "testStrategy": "Stress tests simulating network drops, high latency, and intermittent connectivity. Integration tests to verify automatic reconnection and data consistency after network failures. Concurrency tests during network partitions.",
        "priority": "medium",
        "dependencies": [
          5,
          10
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Implement Authentication & Authorization",
        "description": "Develop robust client authentication (e.g., JWT-based) and server-side row-level security, potentially including a management dashboard for App ID generation.",
        "details": "This involves securing client-server communication, verifying user identities, and enforcing fine-grained access control to data based on user roles and permissions. The management dashboard would facilitate app and user management.",
        "testStrategy": "Security tests for authentication flows (login, token refresh). Integration tests to verify row-level security rules are correctly enforced. Penetration testing for common vulnerabilities.",
        "priority": "medium",
        "dependencies": [
          10
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Implement Storage Optimization (Pruning, Compaction, Caching)",
        "description": "Develop strategies for data pruning, compaction, and advanced caching for both client and server triple stores to optimize storage and performance.",
        "details": "This includes mechanisms to remove old or irrelevant data (pruning), consolidate storage (compaction), and implement multi-level caching strategies to reduce database load and improve query response times.",
        "testStrategy": "Performance tests measuring storage footprint and query times before/after optimization. Stress tests to verify caching effectiveness under high load. Data integrity tests after pruning/compaction.",
        "priority": "low",
        "dependencies": [
          9,
          16
        ],
        "status": "todo",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Develop Framework Integrations (Hooks/Wrappers)",
        "description": "Provide official hooks and wrappers for popular UI frameworks beyond React and Svelte (e.g., Vue, Angular) to simplify integration.",
        "details": "This involves creating framework-specific libraries that abstract away the direct Client API calls, providing idiomatic ways to interact with the system (e.g., custom hooks for data fetching, components for real-time updates).",
        "testStrategy": "Integration tests for each framework-specific wrapper, ensuring correct data flow and reactivity. Develop example applications for each integrated framework. Gather developer feedback.",
        "priority": "low",
        "dependencies": [
          6,
          7
        ],
        "status": "todo",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-08-29T02:04:22.495Z",
      "updated": "2025-08-29T03:22:45.181Z",
      "description": "Tasks for master context"
    }
  }
}